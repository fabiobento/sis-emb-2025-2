## TensorFlow playground

O [_TensorFlow playground_](https://playground.tensorflow.org/)  é um prático simulador de rede neural criado pela equipe do TensorFlow. Neste exercício, você treinará vários classificadores binários em apenas alguns cliques e ajustará a arquitetura do modelo e seus hiperparâmetros para obter alguma intuição sobre como as redes neurais funcionam e o que seus hiperparâmetros fazem. Reserve algum tempo para explorar o seguinte:
1. **Os padrões aprendidos por uma rede neural**. Tente treinar a rede neural padrão clicando no botão __Run__ (canto superior esquerdo). Observe como ela encontra rapidamente uma boa solução para a tarefa de classificação. Os neurônios da primeira camada oculta aprenderam padrões simples, enquanto os neurônios da segunda camada oculta aprenderam a combinar os padrões simples da primeira camada oculta em padrões mais complexos. Em geral, quanto mais camadas houver, mais complexos poderão ser os padrões.
2. **Funções de ativação**. Tente substituir a função de ativação tanh por uma função de ativação ReLU e treine a rede novamente. Observe que ela encontra uma solução ainda mais rapidamente, mas, desta vez, os limites são lineares. Isso se deve à forma da função ReLU
3. **O risco de mínimos locais**. Modifique a arquitetura da rede para ter apenas uma camada oculta com três neurônios. Treine-a várias vezes (para redefinir os pesos da rede, clique no botão __Reset__ ao lado do botão __Run__). Observe que o tempo de treinamento varia muito e, às vezes, ele até fica preso em um mínimo local.
4. **O que acontece quando as redes neurais são muito pequenas**. Remova um neurônio para manter apenas dois. Observe que a rede neural agora é incapaz de encontrar uma boa solução, mesmo que você tente várias vezes. O modelo tem poucos parâmetros e sistematicamente não se ajusta ao conjunto de treinamento.
5. **O risco de __vanishing gradients__ em redes profundas**. Selecione o conjunto de dados __Spiral__ (o conjunto de dados no canto inferior direito em _DATA_) e altere a arquitetura da rede para ter quatro camadas ocultas com oito neurônios cada. Observe que o treinamento é muito mais demorado e frequentemente fica preso em platôs por longos períodos de tempo. Observe também que os neurônios das camadas mais altas (à direita) tendem a evoluir mais rapidamente do que os neurônios das camadas mais baixas (à esquerda). Esse problema, chamado de __vanishing gradients__, pode ser atenuado com uma melhor inicialização dos pesos e outras técnicas, melhores otimizadores (como _AdaGrad_ ou _Adam_) ou normalização de lotes (discutido no tópico 11-Treinando Redes Neurais Profundas).
